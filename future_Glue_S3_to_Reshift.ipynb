{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n%connections futures-historical-glue-redshift-connection",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 50\nConnections to be included:\nfutures-historical-glue-redshift-connection\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nfrom pyspark.sql.functions import *\nfrom awsglue.dynamicframe import DynamicFrame\nfrom awsglue.dynamicframe import DynamicFrame\nfrom awsglue.context import GlueContext\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::378248803116:role/Admin-All-Access-Glue\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 50\nSession ID: e4d595e6-474c-4604-88e1-96580a57e722\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\nWaiting for session e4d595e6-474c-4604-88e1-96580a57e722 to get into ready status...\nSession e4d595e6-474c-4604-88e1-96580a57e722 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import *\nfrom awsglue.dynamicframe import DynamicFrame\nimport boto3",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Get the S3 path to the Parquet files from the Glue job arguments\nS3bucket = \"s3://futures-historical-data\"\n\n# Use boto3 to list the files in the S3 bucket\ns3 = boto3.resource(\"s3\")\nbucket_name = S3bucket[5:].split(\"/\")[0]\nprefix = \"/\".join(S3bucket[5:].split(\"/\")[1:])\ns3_files = [f\"s3://{bucket_name}/{obj.key}\" for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix)]\nparquet_files = [f for f in s3_files if f.endswith(\".parquet\")]",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "for file in parquet_files:\n    # Load the Parquet file into a DynamicFrame\n    dynamic_frame = glueContext.create_dynamic_frame_from_options(\n        connection_type=\"s3\",\n        format=\"parquet\",\n        connection_options={\n            \"paths\": [file]\n        },\n        format_options={\n            \"compression\": \"snappy\"\n        }\n    )\n\n    # Script generated for node Adjust Column Names\n    AdjustColumnNames_node2 = ApplyMapping.apply(\n        frame=dynamic_frame,\n        mappings=[\n            (\"<DATE>\", \"string\", \"Date\", \"varchar\"),\n            (\"<TIME>\", \"string\", \"Time\", \"varchar\"),\n            (\"<OPEN>\", \"double\", \"price_open\", \"double\"),\n            (\"<HIGH>\", \"double\", \"price_high\", \"double\"),\n            (\"<LOW>\", \"double\", \"price_low\", \"double\"),\n            (\"<CLOSE>\", \"double\", \"price_close\", \"double\"),\n            (\"<TICKVOL>\", \"bigint\", \"tickvol\", \"bigint\"),\n            (\"<VOL>\", \"bigint\", \"vol\", \"bigint\"),\n            (\"<SPREAD>\", \"bigint\", \"spread\", \"bigint\"),\n            (\"<INSTRUMENT>\", \"string\", \"instrument\", \"varchar\")\n        ],\n        transformation_ctx=\"AdjustColumnNames_node2\",\n    )\n\n    # Script generated for node Concatenate Columns\n    ConcatenateColumns_node1681862381775 = DynamicFrame.fromDF(\n        AdjustColumnNames_node2.toDF().withColumn(\"datetime\", concat_ws(' ', 'Date', 'Time')),\n        glueContext,\n        'ConverttoDateTime_node1681856295074')\n    \n    # Script generated for node Convert to DateTime\n    ConverttoDateTime_node1681856295074 = DynamicFrame.fromDF(\n        ConcatenateColumns_node1681862381775.toDF().withColumn(\"datetime\", to_timestamp(col('datetime'), 'yyyy.MM.dd HH:mm:ss')),\n        glueContext,\n        'ConverttoDateTime_node1681856295074')\n\n    # Script to drop Date and Time columns\n    DropDateAndTimeColumns = DynamicFrame.fromDF(\n        ConverttoDateTime_node1681856295074.toDF().drop('Date', 'Time'),\n        glueContext,\n        'DropDateAndTimeColumns')\n\n    # Generate a unique table name based on the Parquet file name\n    full_table_name = \"public.futures_\" + file.split(\"/\")[-1].split(\".\")[0]\n    table = full_table_name.split('.')[1]\n    \n    # Write data to Redshift Table\n    AmazonRedshift_node3 = glueContext.write_dynamic_frame.from_options(\n        frame=DropDateAndTimeColumns,\n        connection_type=\"redshift\",\n        connection_options={\n            \"postactions\": f\"BEGIN; MERGE INTO {full_table_name} USING public.futures_temp_7a8472 ON {table}.datetime = futures_temp_7a8472.datetime WHEN MATCHED THEN UPDATE SET price_open = futures_temp_7a8472.price_open, price_high = futures_temp_7a8472.price_high, price_low = futures_temp_7a8472.price_low, price_close = futures_temp_7a8472.price_close, tickvol = futures_temp_7a8472.tickvol, vol = futures_temp_7a8472.vol, spread = futures_temp_7a8472.spread, instrument = futures_temp_7a8472.instrument, datetime = futures_temp_7a8472.datetime WHEN NOT MATCHED THEN INSERT VALUES (futures_temp_7a8472.price_open, futures_temp_7a8472.price_high, futures_temp_7a8472.price_low, futures_temp_7a8472.price_close, futures_temp_7a8472.tickvol, futures_temp_7a8472.vol, futures_temp_7a8472.spread, futures_temp_7a8472.instrument, futures_temp_7a8472.datetime); DROP TABLE public.futures_temp_7a8472; END;\",\n            \"redshiftTmpDir\": \"s3://aws-glue-assets-378248803116-us-east-1/temporary/\",\n            \"useConnectionProperties\": \"true\",\n            \"dbtable\": \"public.futures_temp_7a8472\",\n            \"connectionName\": \"futures-historical-glue-redshift-connection\",\n            \"preactions\": f\"CREATE TABLE IF NOT EXISTS {full_table_name} (price_open DOUBLE PRECISION, price_high DOUBLE PRECISION, price_low DOUBLE PRECISION, price_close DOUBLE PRECISION, tickvol BIGINT, vol BIGINT, spread BIGINT, instrument varchar, datetime TIMESTAMP); DROP TABLE IF EXISTS public.futures_temp_7a8472; CREATE TABLE public.futures_temp_7a8472 (price_open DOUBLE PRECISION, price_high DOUBLE PRECISION, price_low DOUBLE PRECISION, price_close DOUBLE PRECISION, tickvol BIGINT, vol BIGINT, spread BIGINT, instrument varchar, datetime TIMESTAMP);\",\n        },\n        transformation_ctx=\"AmazonRedshift_node3\",\n    )\njob.commit()",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "{\"<DATE>\": \"2001.04.01\", \"<TIME>\": \"17:30:00\", \"<OPEN>\": 0.876, \"<HIGH>\": 0.876, \"<LOW>\": 0.876, \"<CLOSE>\": 0.876, \"<TICKVOL>\": 1, \"<VOL>\": 11, \"<SPREAD>\": 0, \"<INSTRUMENT>\": \"6E\"}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### THIS IS TO TEST THE REDSHIFT CONNECTION ###\n### THIS WILL TAKE A WHILE TO LOAD BASED ON THE TABLE SIZE ###\n\n# from pyspark.sql import SparkSession\n\n# # Replace the values below with your own Redshift connection details\n# jdbc_url = \"jdbc:redshift://futures-historical-cluster.cxdeoy4msdli.us-east-1.redshift.amazonaws.com:5439/futures-historical-redshift-database\"\n# user = \"awsuser\"\n# password = \"ceu-BRM.yqu5dmq9wfj\"\n\n# # Create a SparkSession and set the JDBC connection properties\n# spark = SparkSession.builder.appName(\"myApp\").getOrCreate()\n# df = spark.read \\\n#   .format(\"jdbc\") \\\n#   .option(\"url\", jdbc_url) \\\n#   .option(\"dbtable\", \"public.futures\") \\\n#   .option(\"user\", user) \\\n#   .option(\"password\", password) \\\n#   .option(\"driver\", \"com.amazon.redshift.jdbc.Driver\") \\\n#   .load()\n\n# # Print the DataFrame schema and show a sample of rows\n# df.printSchema()\n# df.show(5)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- price_open: double (nullable = true)\n |-- price_high: double (nullable = true)\n |-- price_low: double (nullable = true)\n |-- price_close: double (nullable = true)\n |-- tickvol: integer (nullable = true)\n |-- vol: integer (nullable = true)\n |-- spread: integer (nullable = true)\n |-- instrument: string (nullable = true)\n |-- datetime: timestamp (nullable = true)\n\n+----------+----------+---------+-----------+-------+----+------+----------+-------------------+\n|price_open|price_high|price_low|price_close|tickvol| vol|spread|instrument|           datetime|\n+----------+----------+---------+-----------+-------+----+------+----------+-------------------+\n|    0.8756|    0.8756|   0.8756|     0.8756|   null|null|  null|        6E|2001-04-01 17:31:00|\n|    0.8756|    0.8756|   0.8754|     0.8754|   null|null|  null|        6E|2001-04-01 17:50:00|\n|    0.8764|    0.8764|   0.8764|     0.8764|   null|null|  null|        6E|2001-04-01 18:20:00|\n|     0.876|     0.876|    0.876|      0.876|   null|null|  null|        6E|2001-04-01 18:55:00|\n|    0.8777|    0.8777|   0.8777|     0.8777|   null|null|  null|        6E|2001-04-01 19:21:00|\n+----------+----------+---------+-----------+-------+----+------+----------+-------------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "hello\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}