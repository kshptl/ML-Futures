{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "quandl_api_key = '_umNYuQHdkCgs9Rcm4Fv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_es = pd.read_csv('/Users/kush/Desktop/EP.csv', sep='\\t')\n",
    "df_nq = pd.read_csv('/Users/kush/Desktop/ENQ.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;DATE&gt;</th>\n",
       "      <th>&lt;TIME&gt;</th>\n",
       "      <th>&lt;OPEN&gt;</th>\n",
       "      <th>&lt;HIGH&gt;</th>\n",
       "      <th>&lt;LOW&gt;</th>\n",
       "      <th>&lt;CLOSE&gt;</th>\n",
       "      <th>&lt;TICKVOL&gt;</th>\n",
       "      <th>&lt;VOL&gt;</th>\n",
       "      <th>&lt;SPREAD&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997.09.08</td>\n",
       "      <td>15:40:00</td>\n",
       "      <td>934.00</td>\n",
       "      <td>934.00</td>\n",
       "      <td>934.00</td>\n",
       "      <td>934.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997.09.09</td>\n",
       "      <td>10:30:00</td>\n",
       "      <td>933.75</td>\n",
       "      <td>934.50</td>\n",
       "      <td>933.75</td>\n",
       "      <td>933.75</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997.09.09</td>\n",
       "      <td>10:31:00</td>\n",
       "      <td>933.75</td>\n",
       "      <td>934.25</td>\n",
       "      <td>933.75</td>\n",
       "      <td>934.25</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997.09.09</td>\n",
       "      <td>10:32:00</td>\n",
       "      <td>934.25</td>\n",
       "      <td>934.75</td>\n",
       "      <td>934.25</td>\n",
       "      <td>934.25</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997.09.09</td>\n",
       "      <td>10:33:00</td>\n",
       "      <td>934.50</td>\n",
       "      <td>934.50</td>\n",
       "      <td>934.50</td>\n",
       "      <td>934.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489280</th>\n",
       "      <td>2023.04.14</td>\n",
       "      <td>20:55:00</td>\n",
       "      <td>4173.50</td>\n",
       "      <td>4173.50</td>\n",
       "      <td>4172.50</td>\n",
       "      <td>4172.50</td>\n",
       "      <td>178</td>\n",
       "      <td>287</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489281</th>\n",
       "      <td>2023.04.14</td>\n",
       "      <td>20:56:00</td>\n",
       "      <td>4172.75</td>\n",
       "      <td>4172.75</td>\n",
       "      <td>4171.75</td>\n",
       "      <td>4171.75</td>\n",
       "      <td>250</td>\n",
       "      <td>3246</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489282</th>\n",
       "      <td>2023.04.14</td>\n",
       "      <td>20:57:00</td>\n",
       "      <td>4171.75</td>\n",
       "      <td>4172.00</td>\n",
       "      <td>4171.50</td>\n",
       "      <td>4171.50</td>\n",
       "      <td>138</td>\n",
       "      <td>378</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489283</th>\n",
       "      <td>2023.04.14</td>\n",
       "      <td>20:58:00</td>\n",
       "      <td>4171.75</td>\n",
       "      <td>4171.75</td>\n",
       "      <td>4171.25</td>\n",
       "      <td>4171.50</td>\n",
       "      <td>187</td>\n",
       "      <td>3123</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489284</th>\n",
       "      <td>2023.04.14</td>\n",
       "      <td>20:59:00</td>\n",
       "      <td>4171.75</td>\n",
       "      <td>4172.25</td>\n",
       "      <td>4171.25</td>\n",
       "      <td>4171.75</td>\n",
       "      <td>242</td>\n",
       "      <td>1454</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7489285 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             <DATE>    <TIME>   <OPEN>   <HIGH>    <LOW>  <CLOSE>  <TICKVOL>  \\\n",
       "0        1997.09.08  15:40:00   934.00   934.00   934.00   934.00          1   \n",
       "1        1997.09.09  10:30:00   933.75   934.50   933.75   933.75        151   \n",
       "2        1997.09.09  10:31:00   933.75   934.25   933.75   934.25         51   \n",
       "3        1997.09.09  10:32:00   934.25   934.75   934.25   934.25        101   \n",
       "4        1997.09.09  10:33:00   934.50   934.50   934.50   934.50          1   \n",
       "...             ...       ...      ...      ...      ...      ...        ...   \n",
       "7489280  2023.04.14  20:55:00  4173.50  4173.50  4172.50  4172.50        178   \n",
       "7489281  2023.04.14  20:56:00  4172.75  4172.75  4171.75  4171.75        250   \n",
       "7489282  2023.04.14  20:57:00  4171.75  4172.00  4171.50  4171.50        138   \n",
       "7489283  2023.04.14  20:58:00  4171.75  4171.75  4171.25  4171.50        187   \n",
       "7489284  2023.04.14  20:59:00  4171.75  4172.25  4171.25  4171.75        242   \n",
       "\n",
       "         <VOL>  <SPREAD>  \n",
       "0            0         0  \n",
       "1            0         0  \n",
       "2            0         0  \n",
       "3            0         0  \n",
       "4            0         0  \n",
       "...        ...       ...  \n",
       "7489280    287        25  \n",
       "7489281   3246        25  \n",
       "7489282    378        25  \n",
       "7489283   3123        25  \n",
       "7489284   1454        25  \n",
       "\n",
       "[7489285 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### S&P ######\n",
    "\n",
    "# Combine the date and time columns into a single datetime column\n",
    "df_es['datetime'] = pd.to_datetime(df_es['<DATE>'] + ' ' + df_es['<TIME>'])\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "df_es = df_es.drop(columns=['<DATE>', '<TIME>', '<TICKVOL>', '<VOL>', '<SPREAD>'])\n",
    "\n",
    "# Rename columns to remove symbols and make them lowercase\n",
    "df_es.columns = [col.replace('<', '').replace('>', '').lower() for col in df_es.columns]\n",
    "df_es = df_es[['datetime', 'open', 'high', 'low', 'close']]\n",
    "\n",
    "# Display the updated dataframe\n",
    "df_es.head()\n",
    "\n",
    "###### NASDAQ ######\n",
    "\n",
    "# Combine the date and time columns into a single datetime column\n",
    "df_nq['datetime'] = pd.to_datetime(df_nq['<DATE>'] + ' ' + df_nq['<TIME>'])\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "df_nq = df_nq.drop(columns=['<DATE>', '<TIME>', '<TICKVOL>', '<VOL>', '<SPREAD>'])\n",
    "\n",
    "# Rename columns to remove symbols and make them lowercase\n",
    "df_nq.columns = [col.replace('<', '').replace('>', '').lower() for col in df_nq.columns]\n",
    "df_nq = df_nq[['datetime', 'open', 'high', 'low', 'close']]\n",
    "\n",
    "# Display the updated dataframe\n",
    "#df_nq.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two datasets based on the datetime column\n",
    "combined_data = pd.merge(df_es, df_nq, on='datetime', suffixes=('_es', '_nq'))\n",
    "\n",
    "# Convert the datetime column to a datetime object\n",
    "combined_data['datetime'] = pd.to_datetime(combined_data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# Select columns 2-10 for normalization\n",
    "norm_cols = combined_data.iloc[:, 1:10].columns.tolist()\n",
    "\n",
    "# Normalize the selected columns\n",
    "scaler = MinMaxScaler()\n",
    "combined_data[norm_cols] = scaler.fit_transform(combined_data[norm_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "timeframes = {'15min' : 100, '5min' : 300, '1min' : 1500}\n",
    "\n",
    "def data_generator(df, timeframes, batch_size):\n",
    "    \"\"\"Generates resampled data based on the specified timeframes and outputs data in the format of X and y.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input dataframe on the 1 min timeframe.\n",
    "        timeframes (dict): The dictionary of timeframes and their corresponding number of timepoints.\n",
    "        batch_size (int): The number of samples to generate per batch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of the X and y arrays in the format of (X, y).\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Initialize X and y arrays\n",
    "        X_15min = []\n",
    "        X_5min = []\n",
    "        X_1min = []\n",
    "        y = []\n",
    "\n",
    "        # Resample the data for each timeframe from the beginning\n",
    "        resampled_data = {}\n",
    "        for timeframe, num_points in timeframes.items():\n",
    "            resampled_data[timeframe] = df.resample(timeframe).apply({'open_es': 'first', 'high_es': 'max', 'low_es': 'min', 'close_es': 'last', 'open_nq': 'first', 'high_nq': 'max', 'low_nq': 'min', 'close_nq': 'last'}).fillna(method='ffill')\n",
    "\n",
    "        # Iterate through each day at 9:30am\n",
    "        for date, _ in df.resample('B'):\n",
    "            # Get the data for the appropriate time intervals for each timeframe for that day at 9:30am\n",
    "            end_time = pd.Timestamp(date) + pd.Timedelta(hours=9, minutes=30)\n",
    "            data = {}\n",
    "            skip_date = False\n",
    "            #every timeframe takes data from the same interval of 1500 minutes back, just at different TFs.\n",
    "            # 1499 instead of 1500 bc if we use 1500, it will include another interval, increasing the timepoint by an additional value\n",
    "            start_time = end_time - pd.Timedelta(minutes=1499) \n",
    "            for timeframe, num_points in timeframes.items():    \n",
    "                data[timeframe] = resampled_data[timeframe].loc[start_time:end_time]\n",
    "                if len(data[timeframe]) < num_points:\n",
    "                    skip_date = True\n",
    "                    break            \n",
    "\n",
    "            if skip_date:\n",
    "                continue\n",
    "\n",
    "            # Combine the resampled data for each timeframe into a single array\n",
    "            resampled_data_combined = [data[timeframe].values for timeframe in timeframes]\n",
    "\n",
    "            # Get the highest high and lowest low between 9:30am and 12 noon for that day\n",
    "            high = df.loc[end_time:end_time+pd.Timedelta(minutes=150)].high_es.max()\n",
    "            low = df.loc[end_time:end_time+pd.Timedelta(minutes=150)].low_es.min()\n",
    "\n",
    "            # Add the resampled data and high/low values to the X and y arrays\n",
    "            X_15min.append(resampled_data_combined[0])\n",
    "            X_5min.append(resampled_data_combined[1])\n",
    "            X_1min.append(resampled_data_combined[2])\n",
    "            y.append((high, low))\n",
    "\n",
    "            # Generate a new batch if the current batch is full\n",
    "            if len(X_1min) == batch_size:\n",
    "                yield ([np.array(X_15min), np.array(X_5min), np.array(X_1min)], np.array(y))\n",
    "                X_15min = []\n",
    "                X_5min = []\n",
    "                X_1min = []\n",
    "                y = []\n",
    "\n",
    "        # Yield the final batch if there are any remaining samples\n",
    "        #if len(X_1min) > 0:\n",
    "        #    yield ([np.array(X_15min), np.array(X_5min), np.array(X_1min)], np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "# Define the input layers\n",
    "input_15min = Input(shape=(100, 8, 1), name='15min')\n",
    "input_5min = Input(shape=(300, 8, 1), name='5min')\n",
    "input_1min = Input(shape=(1500, 8, 1), name='1min')\n",
    "\n",
    "# Define the convolutional layers with glorot_uniform initialization\n",
    "conv_15min = Conv2D(filters=16, kernel_size=(5, 1), activation='relu', kernel_initializer=glorot_uniform(seed=1))(input_15min)\n",
    "conv_5min = Conv2D(filters=16, kernel_size=(15, 1), activation='relu', kernel_initializer=glorot_uniform(seed=2))(input_5min)\n",
    "conv_1min = Conv2D(filters=16, kernel_size=(50, 1), activation='relu', kernel_initializer=glorot_uniform(seed=3))(input_1min)\n",
    "\n",
    "# Flatten the convolutional layers\n",
    "flatten_15min = Flatten()(conv_15min)\n",
    "flatten_5min = Flatten()(conv_5min)\n",
    "flatten_1min = Flatten()(conv_1min)\n",
    "\n",
    "# Concatenate the flattened layers\n",
    "concatenated = Concatenate()([flatten_15min, flatten_5min, flatten_1min])\n",
    "\n",
    "# Add a dense layer\n",
    "dense_layer = Dense(32, activation='relu', kernel_initializer=glorot_uniform(seed=4))(concatenated)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(2, activation='linear', name='output', kernel_initializer=glorot_uniform(seed=5))(dense_layer)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_15min, input_5min, input_1min], outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:55:06.478293: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 10s 138ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/2\n",
      "73/73 [==============================] - 10s 133ms/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 2\n",
    "\n",
    "# Compile the model with a learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the data generator\n",
    "train_generator = data_generator(combined_data.loc['2012':'2020'], timeframes, batch_size)\n",
    "val_generator = data_generator(combined_data.loc['2021':], timeframes, batch_size)\n",
    "\n",
    "# Define the early stopping criteria\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model using the generator\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch= 2348 // batch_size,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps= 858 // batch_size,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = data_generator(combined_data.loc['2012':'2020'], timeframes, 1)\n",
    "\n",
    "for i in range(1):\n",
    "    X, y = next(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model that outputs the output of each layer\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# Get the output of each layer for a sample input\n",
    "sample_input = X\n",
    "activations = activation_model.predict(sample_input)\n",
    "print(y)\n",
    "\n",
    "# Print the output of each layer\n",
    "for i, activation in enumerate(activations):\n",
    "    print(f'Layer {i}:')\n",
    "    print(activation.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== LAYER:  15min  =====\n",
      "weights:  []\n",
      "===== LAYER:  5min  =====\n",
      "weights:  []\n",
      "===== LAYER:  1min  =====\n",
      "weights:  []\n",
      "===== LAYER:  conv2d_3  =====\n",
      "weights:\n",
      "nan\n",
      "biases:\n",
      "[nan  0. nan nan  0.  0. nan  0.  0. nan nan  0.  0. nan nan nan]\n",
      "===== LAYER:  conv2d_4  =====\n",
      "weights:\n",
      "nan\n",
      "biases:\n",
      "[ 0.  0. nan  0.  0.  0. nan  0.  0. nan nan  0.  0.  0.  0. nan]\n",
      "===== LAYER:  conv2d_5  =====\n",
      "weights:\n",
      "nan\n",
      "biases:\n",
      "[nan nan nan nan nan nan  0. nan nan  0.  0. nan  0.  0.  0.  0.]\n",
      "===== LAYER:  flatten_3  =====\n",
      "weights:  []\n",
      "===== LAYER:  flatten_4  =====\n",
      "weights:  []\n",
      "===== LAYER:  flatten_5  =====\n",
      "weights:  []\n",
      "===== LAYER:  concatenate_1  =====\n",
      "weights:  []\n",
      "===== LAYER:  dense_1  =====\n",
      "weights:\n",
      "nan\n",
      "biases:\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "===== LAYER:  output  =====\n",
      "weights:\n",
      "nan\n",
      "biases:\n",
      "[nan nan]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(\"===== LAYER: \", layer.name, \" =====\")\n",
    "    if layer.get_weights() != []:\n",
    "        weights = layer.get_weights()[0].mean()\n",
    "        biases = layer.get_weights()[1]\n",
    "        print(\"weights:\")\n",
    "        print(weights)\n",
    "        print(\"biases:\")\n",
    "        print(biases)\n",
    "    else:\n",
    "        print(\"weights: \", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.039047010242938995, nan, nan, nan, nan, nan, nan, nan, nan, nan]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "loss_values = []\n",
    "\n",
    "for step in range(10):\n",
    "    # Generate a batch of data\n",
    "    X_batch, y_batch = next(train_generator)\n",
    "\n",
    "    # Train the model on the batch of data\n",
    "    loss = model.train_on_batch(X_batch, y_batch)\n",
    "\n",
    "    # Append the loss value to the loss_values array\n",
    "    loss_values.append(loss)\n",
    "\n",
    "loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
