{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mplfinance as mpf\n",
    "import re\n",
    "import math\n",
    "from math import floor\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import dask\n",
    "\n",
    "from functions import *\n",
    "from functions.preprocessing import *\n",
    "from functions.swing_points import *\n",
    "from functions.fvg import *\n",
    "\n",
    "quandl_api_key = '_umNYuQHdkCgs9Rcm4Fv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_es = pd.read_csv('es-1m_bk.csv', sep=';', names=['date', 'time', 'open', 'high', 'low', 'close', 'vol'])\n",
    "df_es['datetime'] = pd.to_datetime(df_es.date + ' ' + df_es.time, format='%d/%m/%Y %H:%M')\n",
    "df_es = df_es.infer_objects()\n",
    "df_es.set_index(df_es.datetime, inplace=True)\n",
    "df_es.index = df_es.index.tz_localize('America/Chicago').tz_convert('America/New_York')\n",
    "df_es.drop(columns=['date', 'time', 'datetime', 'vol'], inplace=True)\n",
    "df_es = df_es[~df_es.index.duplicated(keep='last')].astype('float32')\n",
    "\n",
    "df_nq = pd.read_csv('nq-1m_bk.csv', sep=';', names=['date', 'time', 'open', 'high', 'low', 'close', 'vol'])\n",
    "df_nq['datetime'] = pd.to_datetime(df_nq.date + ' ' + df_nq.time, format='%d/%m/%Y %H:%M')\n",
    "df_nq = df_nq.infer_objects()\n",
    "df_nq.set_index(df_nq.datetime, inplace=True)\n",
    "df_nq.index = df_nq.index.tz_localize('America/Chicago').tz_convert('America/New_York')\n",
    "df_nq.drop(columns=['date', 'time', 'datetime', 'vol'], inplace=True)\n",
    "df_nq = df_nq[~df_nq.index.duplicated(keep='last')].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = False\n",
    "if test:\n",
    "    df_es = pd.read_csv('EP_test_set.csv', sep='\\t')\n",
    "    df_nq = pd.read_csv('ENQ_test_set.csv', sep='\\t')\n",
    "    df_es = preprocess_dataframe(df_es, log_returns=False)\n",
    "    df_nq = preprocess_dataframe(df_nq, log_returns=False)\n",
    "else:    \n",
    "    df_es = pd.read_csv('/Users/kush/Desktop/futures_data/EP.csv', low_memory=True)\n",
    "    df_nq = pd.read_csv('/Users/kush/Desktop/futures_data/ENQ.csv', low_memory=True)\n",
    "    \n",
    "    rows = 100000000#max(len(df_es), len(df_nq)) // 2\n",
    "    df_es = df_es[-rows:]\n",
    "    df_nq = df_nq[-rows:]\n",
    "    \n",
    "    df_es = preprocess_dataframe(df_es, log_returns=False)\n",
    "    df_nq = preprocess_dataframe(df_nq, log_returns=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding swing points\n",
      "Getting past 20 swings into list\n",
      "Dropping unnecesary swings columns\n",
      "Converting dtypes\n",
      "Getting Fair Value Gaps\n",
      "Getting past 20 FVGs\n",
      "Dropping unnecesary FVG columns\n",
      "Converting dtypes\n",
      "Adding DF to list \n",
      "\n",
      "Finding swing points\n",
      "Getting past 20 swings into list\n",
      "Dropping unnecesary swings columns\n",
      "Converting dtypes\n",
      "Getting Fair Value Gaps\n",
      "Getting past 20 FVGs\n",
      "Dropping unnecesary FVG columns\n",
      "Converting dtypes\n",
      "Adding DF to list \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "global past_swings\n",
    "\n",
    "for df in [df_es, df_nq]:\n",
    "    log_returns=False\n",
    "\n",
    "    # Step 1: Get swing points for each window\n",
    "    lookback = 20\n",
    "    timeframes = ['1T', '5T', '15T', '60T', '240T', '1440T']\n",
    "    print(f'Finding swing points')\n",
    "    df = identify_swing_points_optimized(df, timeframes, interpolation_method='None')\n",
    "\n",
    "    # Step 2: Get a list of the past {lookback} swing points for each time point\n",
    "    print(f'Getting past {lookback} swings into list')\n",
    "    df = swings_to_features_optimized(df, lookback, log_returns)\n",
    "    \n",
    "    # Step 4: Drop columns from Step 1 and Step 2\n",
    "    print('Dropping unnecesary swings columns')\n",
    "    regex_swings = r'^swing_high_\\d{1,4}[TDW]$|^swing_low_\\d{1,4}[TDW]$'\n",
    "    pivot_cols_to_drop = [c for c in df.columns if re.match(regex_swings, c)]\n",
    "    df = df.drop(columns=pivot_cols_to_drop)\n",
    "    \n",
    "    # convert cols dtype to save memory\n",
    "    df.fillna(0, inplace=True)\n",
    "    print('Converting dtypes')\n",
    "    cols_to_float32 = [c for c in df.columns if any(s in c for s in ['open', 'high', 'low', 'close'])]\n",
    "    df[cols_to_float32] = df[cols_to_float32].astype('float32')\n",
    "\n",
    "    # Step 5: Get Fair Value Gaps for each timeframe\n",
    "    print('Getting Fair Value Gaps')\n",
    "    timeframes_fvg = ['1T', '5T', '15T', '60T', '240T', '1440T']\n",
    "    df = identify_fair_value_gaps_optimized(df, timeframes_fvg)\n",
    "\n",
    "    # Step 6: Get list of past {lookback} FVGs\n",
    "    print(f'Getting past {lookback} FVGs')\n",
    "    df = fvg_to_feature_optimized(df, lookback, log_returns)\n",
    "    \n",
    "     # Step 8:   Drop columns from Step 6 and Step 6\n",
    "    print('Dropping unnecesary FVG columns')\n",
    "    regex_fvg = r'^fair_value_gap_\\d{1,4}[TDW]$|^fair_value_gap_\\d{1,4}[TDW]_high$|^fair_value_gap_\\d{1,4}[TDW]_low$'\n",
    "    fvg_cols_to_drop = [c for c in df.columns if re.match(regex_fvg, c)]\n",
    "    df = df.drop(columns=fvg_cols_to_drop)\n",
    "\n",
    "    # convert cols dtype to save memory\n",
    "    df.fillna(0, inplace=True)\n",
    "    print('Converting dtypes')\n",
    "    cols_to_float32 = [c for c in df.columns if any(s in c for s in ['open', 'high', 'low', 'close'])]\n",
    "    df[cols_to_float32] = df[cols_to_float32].astype('float32')\n",
    "    \n",
    "    cols_to_uint16 = [c for c in df.columns if re.match(r'^fair_value_gap_\\d{1,4}[TDW]_\\d+$', c)]\n",
    "    df[cols_to_uint16] = df[cols_to_uint16].astype('uint16')\n",
    "\n",
    "    print('Adding DF to list \\n')\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATAFRAME IS READY, NEXT STEP IS TO DO THE MACHINE LEARNING\n",
    "# TRY IT LOCALLY THEN GO TO SAGEMAKER (DEPENDING ON HOW LOCALLY GOES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fair_value_gap_1T_1',\n",
       " 'fair_value_gap_1T_2',\n",
       " 'fair_value_gap_1T_3',\n",
       " 'fair_value_gap_1T_4',\n",
       " 'fair_value_gap_1T_5',\n",
       " 'fair_value_gap_1T_6',\n",
       " 'fair_value_gap_1T_7',\n",
       " 'fair_value_gap_1T_8',\n",
       " 'fair_value_gap_1T_9',\n",
       " 'fair_value_gap_1T_10',\n",
       " 'fair_value_gap_1T_11',\n",
       " 'fair_value_gap_1T_12',\n",
       " 'fair_value_gap_1T_13',\n",
       " 'fair_value_gap_1T_14',\n",
       " 'fair_value_gap_1T_15',\n",
       " 'fair_value_gap_1T_16',\n",
       " 'fair_value_gap_1T_17',\n",
       " 'fair_value_gap_1T_18',\n",
       " 'fair_value_gap_1T_19',\n",
       " 'fair_value_gap_1T_20',\n",
       " 'fair_value_gap_5T_1',\n",
       " 'fair_value_gap_5T_2',\n",
       " 'fair_value_gap_5T_3',\n",
       " 'fair_value_gap_5T_4',\n",
       " 'fair_value_gap_5T_5',\n",
       " 'fair_value_gap_5T_6',\n",
       " 'fair_value_gap_5T_7',\n",
       " 'fair_value_gap_5T_8',\n",
       " 'fair_value_gap_5T_9',\n",
       " 'fair_value_gap_5T_10',\n",
       " 'fair_value_gap_5T_11',\n",
       " 'fair_value_gap_5T_12',\n",
       " 'fair_value_gap_5T_13',\n",
       " 'fair_value_gap_5T_14',\n",
       " 'fair_value_gap_5T_15',\n",
       " 'fair_value_gap_5T_16',\n",
       " 'fair_value_gap_5T_17',\n",
       " 'fair_value_gap_5T_18',\n",
       " 'fair_value_gap_5T_19',\n",
       " 'fair_value_gap_5T_20',\n",
       " 'fair_value_gap_15T_1',\n",
       " 'fair_value_gap_15T_2',\n",
       " 'fair_value_gap_15T_3',\n",
       " 'fair_value_gap_15T_4',\n",
       " 'fair_value_gap_15T_5',\n",
       " 'fair_value_gap_15T_6',\n",
       " 'fair_value_gap_15T_7',\n",
       " 'fair_value_gap_15T_8',\n",
       " 'fair_value_gap_15T_9',\n",
       " 'fair_value_gap_15T_10',\n",
       " 'fair_value_gap_15T_11',\n",
       " 'fair_value_gap_15T_12',\n",
       " 'fair_value_gap_15T_13',\n",
       " 'fair_value_gap_15T_14',\n",
       " 'fair_value_gap_15T_15',\n",
       " 'fair_value_gap_15T_16',\n",
       " 'fair_value_gap_15T_17',\n",
       " 'fair_value_gap_15T_18',\n",
       " 'fair_value_gap_15T_19',\n",
       " 'fair_value_gap_15T_20',\n",
       " 'fair_value_gap_60T_1',\n",
       " 'fair_value_gap_60T_2',\n",
       " 'fair_value_gap_60T_3',\n",
       " 'fair_value_gap_60T_4',\n",
       " 'fair_value_gap_60T_5',\n",
       " 'fair_value_gap_60T_6',\n",
       " 'fair_value_gap_60T_7',\n",
       " 'fair_value_gap_60T_8',\n",
       " 'fair_value_gap_60T_9',\n",
       " 'fair_value_gap_60T_10',\n",
       " 'fair_value_gap_60T_11',\n",
       " 'fair_value_gap_60T_12',\n",
       " 'fair_value_gap_60T_13',\n",
       " 'fair_value_gap_60T_14',\n",
       " 'fair_value_gap_60T_15',\n",
       " 'fair_value_gap_60T_16',\n",
       " 'fair_value_gap_60T_17',\n",
       " 'fair_value_gap_60T_18',\n",
       " 'fair_value_gap_60T_19',\n",
       " 'fair_value_gap_60T_20',\n",
       " 'fair_value_gap_240T_1',\n",
       " 'fair_value_gap_240T_2',\n",
       " 'fair_value_gap_240T_3',\n",
       " 'fair_value_gap_240T_4',\n",
       " 'fair_value_gap_240T_5',\n",
       " 'fair_value_gap_240T_6',\n",
       " 'fair_value_gap_240T_7',\n",
       " 'fair_value_gap_240T_8',\n",
       " 'fair_value_gap_240T_9',\n",
       " 'fair_value_gap_240T_10',\n",
       " 'fair_value_gap_240T_11',\n",
       " 'fair_value_gap_240T_12',\n",
       " 'fair_value_gap_240T_13',\n",
       " 'fair_value_gap_240T_14',\n",
       " 'fair_value_gap_240T_15',\n",
       " 'fair_value_gap_240T_16',\n",
       " 'fair_value_gap_240T_17',\n",
       " 'fair_value_gap_240T_18',\n",
       " 'fair_value_gap_240T_19',\n",
       " 'fair_value_gap_240T_20',\n",
       " 'fair_value_gap_1440T_1',\n",
       " 'fair_value_gap_1440T_2',\n",
       " 'fair_value_gap_1440T_3',\n",
       " 'fair_value_gap_1440T_4',\n",
       " 'fair_value_gap_1440T_5',\n",
       " 'fair_value_gap_1440T_6',\n",
       " 'fair_value_gap_1440T_7',\n",
       " 'fair_value_gap_1440T_8',\n",
       " 'fair_value_gap_1440T_9',\n",
       " 'fair_value_gap_1440T_10',\n",
       " 'fair_value_gap_1440T_11',\n",
       " 'fair_value_gap_1440T_12',\n",
       " 'fair_value_gap_1440T_13',\n",
       " 'fair_value_gap_1440T_14',\n",
       " 'fair_value_gap_1440T_15',\n",
       " 'fair_value_gap_1440T_16',\n",
       " 'fair_value_gap_1440T_17',\n",
       " 'fair_value_gap_1440T_18',\n",
       " 'fair_value_gap_1440T_19',\n",
       " 'fair_value_gap_1440T_20']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_list[0]\n",
    "df_list[0].select_dtypes(include='uint16').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# combine dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_es\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_nq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#del(df_es)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#del(df_nq)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# extract year, month, day, hour, minute, and day of week as separate reatures.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39massign(\n\u001b[1;32m      8\u001b[0m     year\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39myear,\n\u001b[1;32m      9\u001b[0m     month\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmonth,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     day_of_the_week\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mdayofweek \u001b[38;5;66;03m#pd.get_dummies (below) removes the 'day_of_week' column to one-hot-encode, but we still need it for the df_filtered code.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/DS/env/lib/python3.10/site-packages/pandas/core/reshape/merge.py:158\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    143\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    144\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[1;32m    145\u001b[0m         left,\n\u001b[1;32m    146\u001b[0m         right,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m    157\u001b[0m     )\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DS/env/lib/python3.10/site-packages/pandas/core/reshape/merge.py:807\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m    805\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[0;32m--> 807\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[0;32m~/Documents/DS/env/lib/python3.10/site-packages/pandas/core/reshape/merge.py:782\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[0;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[1;32m    777\u001b[0m left\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m join_index\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m right_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(\n\u001b[1;32m    780\u001b[0m     right_indexer, \u001b[38;5;28mlen\u001b[39m(right)\n\u001b[1;32m    781\u001b[0m ):\n\u001b[0;32m--> 782\u001b[0m     rmgr \u001b[38;5;241m=\u001b[39m \u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    791\u001b[0m     right \u001b[38;5;241m=\u001b[39m right\u001b[38;5;241m.\u001b[39m_constructor(rmgr)\n\u001b[1;32m    792\u001b[0m right\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m join_index\n",
      "File \u001b[0;32m~/Documents/DS/env/lib/python3.10/site-packages/pandas/core/internals/managers.py:747\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    740\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m    741\u001b[0m         indexer,\n\u001b[1;32m    742\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m    743\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[1;32m    744\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[1;32m    745\u001b[0m     )\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 747\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    748\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[1;32m    749\u001b[0m             indexer,\n\u001b[1;32m    750\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    751\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    752\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[1;32m    753\u001b[0m             ),\n\u001b[1;32m    754\u001b[0m         )\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    756\u001b[0m     ]\n\u001b[1;32m    758\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m    759\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[0;32m~/Documents/DS/env/lib/python3.10/site-packages/pandas/core/internals/managers.py:748\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    740\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m    741\u001b[0m         indexer,\n\u001b[1;32m    742\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m    743\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[1;32m    744\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[1;32m    745\u001b[0m     )\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 748\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    756\u001b[0m     ]\n\u001b[1;32m    758\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m    759\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[0;32m~/Documents/DS/env/lib/python3.10/site-packages/pandas/core/internals/blocks.py:945\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m    942\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DS/env/lib/python3.10/site-packages/pandas/core/array_algos/take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[1;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DS/env/lib/python3.10/site-packages/pandas/core/array_algos/take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[1;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[1;32m    161\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[1;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# combine dataset\n",
    "df = pd.merge(df_list[0], df_list[1], 'inner', 'datetime', suffixes=('_es', '_nq'))\n",
    "#del(df_es)\n",
    "#del(df_nq)\n",
    "\n",
    "# extract year, month, day, hour, minute, and day of week as separate reatures.\n",
    "df = df.assign(\n",
    "    year=df.index.year,\n",
    "    month=df.index.month,\n",
    "    day=df.index.day,\n",
    "    date=df.index.date,\n",
    "    hour=df.index.hour,\n",
    "    minute=df.index.minute,\n",
    "    day_of_week=df.index.dayofweek,\n",
    "    day_of_the_week=df.index.dayofweek #pd.get_dummies (below) removes the 'day_of_week' column to one-hot-encode, but we still need it for the df_filtered code.\n",
    ")\n",
    "df['day_of_the_week'] = df['day_of_week']\n",
    "df = pd.get_dummies(df, 'day_of_week', columns=['day_of_week'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataframe is fragemented, t de-fragment it use copy.\n",
    "#df = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CALUCLATE NWOGS and get add past 5 to each time point #####\n",
    "\n",
    "# Resample to daily frequency and backfill the OHLC into the weekends.\n",
    "daily_df = df.resample('D').agg({'open_es': 'first', 'high_es' : 'max', 'low_es' : 'min', 'close_es': 'last'}).bfill()\n",
    "\n",
    "# Identify closing prices on Fridays\n",
    "friday_closes = daily_df[daily_df.index.dayofweek == 4]['close_es']\n",
    "\n",
    "# Identify opening prices on Sundays\n",
    "# since we backfilled, the next market opening price will be the opening price on saturday. shift it up one to match the friday index\n",
    "# then we have the friday close and next market open on the same index.\n",
    "sunday_opening = daily_df['open_es'].shift(-1) #\n",
    "sunday_opening = sunday_opening[sunday_opening.index.dayofweek == 4]\n",
    "\n",
    "# convert to dataframe and get log returns\n",
    "if log_returns:\n",
    "    nwog_df = pd.DataFrame({'NWOG_close': np.log(friday_closes / friday_closes.shift(1)), 'NWOG_open': np.log(sunday_opening / sunday_opening.shift(1))})\n",
    "else:\n",
    "    nwog_df = pd.DataFrame({'NWOG_close': friday_closes, 'NWOG_open': sunday_opening})\n",
    "\n",
    "# Create columns for the past 5 closes and openings\n",
    "for i in range(1, 6):\n",
    "    nwog_df[f'NWOG_close_{i}'] = nwog_df['NWOG_close'].shift(i)\n",
    "    nwog_df[f'NWOG_open_{i}'] = nwog_df['NWOG_open'].shift(i)\n",
    "\n",
    "# Drop the original 'NWOG_close' and 'NWOG_open' columns\n",
    "nwog_df.drop(columns=['NWOG_close', 'NWOG_open'], inplace=True)\n",
    "\n",
    "df = df.join(nwog_df, how='left').fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DAILY HIGH AND LOW BEFORE LUNCH\n",
    "\n",
    "# Filter data, calculate daily high and low and their times, and merge back to original DataFrame\n",
    "filtered_df = df[df.day_of_the_week < 5].between_time('09:30', '11:59')\n",
    "morning_high_low = filtered_df.groupby('date').agg({'high_es': 'max', 'low_es': 'min'})\n",
    "morning_high_low.columns = ['Morning_High', 'Morning_Low']\n",
    "\n",
    "# Get Daily High and Low\n",
    "filtered_df = df[df.day_of_the_week < 5].between_time('09:30', '15:59')\n",
    "daily_high_low = filtered_df.groupby('date').agg({'high_es': 'max', 'low_es': 'min'})\n",
    "daily_high_low.columns = ['Daily_High', 'Daily_Low']\n",
    "morning_high_low = morning_high_low.join(daily_high_low)\n",
    "\n",
    "# Get Daily Midnight Open\n",
    "morning_high_low = morning_high_low.join(df[(df.hour == 0) & (df.minute == 0)][['open_es', 'date']].set_index('date')).rename({'open_es' : 'Midnight_Open'}, axis='columns')\n",
    "\n",
    "df = df.merge(morning_high_low, how='left', on='date')\n",
    "\n",
    "# GET ONLY 0929 TIMEPOINTS FOR EACH DAY\n",
    "df_final = df[(df.hour == 9) & (df.minute == 29)].reset_index().dropna()\n",
    "\n",
    "# convert daily high and low and midnight variables to log return of the close price at 9:29\n",
    "#df_final[['Daily_High', 'Daily_Low']] = df_final[['Daily_High', 'Daily_Low']].div(df_final.close_es, axis=0).apply(np.log)\n",
    "#df_final[['Morning_High', 'Morning_Low']] = df_final[['Morning_High', 'Morning_Low']].div(df_final.close_es, axis=0).apply(np.log)\n",
    "#df_final['Midnight_Open'] = df_final['Midnight_Open'].div(df_final.close_es, axis=0).apply(np.log)\n",
    "\n",
    "# convert the rest of the price columns to log returns of the close price at 9:29\n",
    "price_cols = [c for c in df_final.columns if any(s in c for s in ['swing', 'fair', 'NWOG'])]\n",
    "df_final[price_cols] = df_final[price_cols].div(df_final.close_es, axis=0).apply(np.log)\n",
    "\n",
    "\n",
    "# convert OHLC log returns based on previous day's 0929 candle\n",
    "#df_final = calculate_log_returns(df_final, ['open_es', 'high_es', 'low_es', 'close_es'])\n",
    "#df_final = calculate_log_returns(df_final, ['open_nq', 'high_nq', 'low_nq', 'close_nq'])\n",
    "\n",
    "\n",
    "# Transform 'month' variable (max value is 12)\n",
    "df_final['month_sin'] = np.sin(2 * np.pi * df_final['month'] / 12)\n",
    "df_final['month_cos'] = np.cos(2 * np.pi * df_final['month'] / 12)\n",
    "\n",
    "# Transform 'day' variable (max value is 31)\n",
    "df_final['day_sin'] = np.sin(2 * np.pi * df_final['day'] / 31)\n",
    "df_final['day_cos'] = np.cos(2 * np.pi * df_final['day'] / 31)\n",
    "\n",
    "# Drop unnecesary columns\n",
    "df_final.drop(columns=['day_of_week_6', 'day_of_the_week', 'date', 'hour', 'minute', 'index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('df_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
