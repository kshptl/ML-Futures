{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f0aa5e-d829-409e-ac8d-8762a5212a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c9db98-e7a7-4869-a61b-916423d8ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mplfinance as mpf\n",
    "import re\n",
    "import math\n",
    "from math import floor\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime as dt\n",
    "\n",
    "from dain import Adaptive_Normalizer_Layer\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import *#Reshape, LSTM, Dense, Bidirectional, GRU, Dropout, Input, Embedding, MultiHeadAttention, LayerNormalization, Conv1D, GlobalMaxPooling1D\n",
    "from fit_one_cycle import OneCycleScheduler\n",
    "from lr_finder import LRFinder\n",
    "\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "\n",
    "df_1T = pd.read_parquet('df_CNN_LSTM_1T.parquet').reset_index().reset_index().set_index('datetime', drop=True)\n",
    "df_5T = pd.read_parquet('df_CNN_LSTM_5T.parquet')\n",
    "df_15T = pd.read_parquet('df_CNN_LSTM_15T.parquet')\n",
    "df_1H = pd.read_parquet('df_CNN_LSTM_1H.parquet')\n",
    "df_4H = pd.read_parquet('df_CNN_LSTM_4H.parquet')\n",
    "df_1B = pd.read_parquet('df_CNN_LSTM_1B.parquet').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b062716-0eb2-49f8-90f3-0f4bce59bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data generator function\n",
    "def data_generator(data, window_size, batch_size, indices, test=False, shap=False):\n",
    "    while True:\n",
    "        #global batch_data\n",
    "        #global input_sequences\n",
    "        #global index\n",
    "        batch_data = []\n",
    "        batch_targets = []\n",
    "        for index in indices:\n",
    "            dt_index = df_1T.iloc[index].name\n",
    "\n",
    "            input_sequences = np.array([\n",
    "\n",
    "            df_1T.loc[:dt_index-pd.Timedelta(minutes=1)][-360:].fillna(0).drop(columns='index').values.astype('float32'),\n",
    "            df_5T.loc[:dt_index-pd.Timedelta(minutes=5)][-360:].fillna(0).values.astype('float32'),\n",
    "            df_15T.loc[:dt_index-pd.Timedelta(minutes=15)][-360:].fillna(0).values.astype('float32'),\n",
    "            df_4H.loc[:dt_index-pd.Timedelta(hours=4)][-360:].fillna(0).values.astype('float32'),\n",
    "            df_4H.loc[:dt_index-pd.Timedelta(hours=4)][-360:].fillna(0).values.astype('float32'),\n",
    "            df_1B.loc[:dt_index.normalize()-pd.Timedelta(days=1)][-360:].fillna(0).values.astype('float32')\n",
    "\n",
    "            ])\n",
    "\n",
    "            # Extract the target high and low values\n",
    "            target_high = df_1T[index : index+151].high_es.max()\n",
    "            target_low = df_1T[index : index+151].low_es.min()\n",
    "\n",
    "            # Add the input sequence and targets to the batch\n",
    "            batch_data.append(input_sequences)\n",
    "            batch_targets.append([target_high, target_low])\n",
    "            #print(f'batch_data shape: {len(batch_data), batch_data[-1].shape}')\n",
    "\n",
    "            #print([target_high, target_low], index)\n",
    "            if len(batch_data) == batch_size:\n",
    "                \n",
    "                # Convert to np.array\n",
    "                batch_data = np.asarray(batch_data)\n",
    "                batch_targets = np.asarray(batch_targets)\n",
    "                #print(f'final batch shape {batch_data.shape, type(batch_data[0])}')\n",
    "                #sys.exit()\n",
    "                # Normalize\n",
    "                for i in range(batch_data.shape[1]):\n",
    "                    a = batch_data[:, i, :, :]\n",
    "                    scaler = StandardScaler()\n",
    "                    batch_data[:, i, :, :] = scaler.fit_transform(a.reshape(-1, a.shape[-1])).reshape(a.shape[0], a.shape[1], a.shape[2])\n",
    "                \n",
    "                batch_data = batch_data.astype('float32')\n",
    "                batch_data = np.split(batch_data, 6, axis=1)\n",
    "                batch_data = [element.squeeze(axis=1) for element in batch_data]\n",
    "                \n",
    "                if test:\n",
    "                    yield batch_data, np.asarray(batch_targets).astype('float32')\n",
    "                elif shap:\n",
    "                    yield tf.convert_to_tensor(np.asarray(batch_data), np.asarray(batch_targets).astype('float32'))\n",
    "                else:\n",
    "                    yield batch_data, np.asarray(batch_targets).astype('float32')\n",
    "                    \n",
    "                batch_data = []\n",
    "                batch_targets = []\n",
    "                \n",
    "        # Yield the remaining batch if it doesn't equal the batch size\n",
    "        if batch_data:\n",
    "         # Convert to np.array\n",
    "            batch_data = np.asarray(batch_data)\n",
    "            batch_targets = np.asarray(batch_targets)\n",
    "            \n",
    "            # Normalize\n",
    "            for i in range(batch_data.shape[1]):\n",
    "                a = batch_data[:, i, :, :]\n",
    "                scaler = StandardScaler()\n",
    "                batch_data[:, i, :, :] = scaler.fit_transform(a.reshape(-1, a.shape[-1])).reshape(a.shape[0], a.shape[1], a.shape[2])\n",
    "            \n",
    "            batch_data = batch_data.astype('float32')\n",
    "            batch_data = np.split(batch_data, 6, axis=1)\n",
    "            batch_data = [element.squeeze(axis=1) for element in batch_data]\n",
    "            \n",
    "            if test:\n",
    "                yield batch_data, np.asarray(batch_targets).astype('float32')\n",
    "            elif shap:\n",
    "                yield tf.convert_to_tensor(np.asarray(batch_data), np.asarray(batch_targets))\n",
    "            else:\n",
    "                yield batch_data, np.asarray(batch_targets).astype('float32')\n",
    "        \n",
    "        batch_data = []\n",
    "        batch_targets = []    \n",
    "                \n",
    "# Set the window size and batch size\n",
    "window_size = 362880 # minutes in 252 days (1 year of trading days)\n",
    "batch_size = 32\n",
    "num_features = df_1T.shape[1]-1\n",
    "\n",
    "# Set the split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "\n",
    "# set random seed for the shuffle function\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get the indices for training, validation, and testing splits\n",
    "# Filter the indices to exclude those with insufficient historical data\n",
    "indices = df_1T.loc[(df_1T.index.hour == 9) & (df_1T.index.minute == 30) & (df_1T.index.dayofweek < 5) & (df_1T['index'] > 681587)]['index'].to_numpy()\n",
    "np.random.shuffle(indices)\n",
    "train_size = int(len(indices) * train_ratio)\n",
    "val_size = int(len(indices) * val_ratio)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "#get the mean and std for normalization\n",
    "#price_cols_to_normalize = list(set(df_dropped.columns[(df_dropped>1).any()]) - set(['weekly_open', 'weekly_close', 'daily_open']))\n",
    "\n",
    "# Create the data generators for training, validation, and testing\n",
    "train_generator = data_generator(df_1T, window_size, batch_size, train_indices)\n",
    "val_generator = data_generator(df_1T, window_size, batch_size, val_indices)\n",
    "test_generator = data_generator(df_1T, window_size, batch_size, test_indices)\n",
    "\n",
    "# Calculate the number of steps per epoch for training and validation\n",
    "train_steps_per_epoch = len(train_indices) // batch_size\n",
    "val_steps = len(val_indices) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7238b97a-b5d1-4a95-9d33-7f62da532324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    }
   ],
   "source": [
    "# Define the Transformer model\n",
    "\n",
    "# Set the window size and number of features\n",
    "\n",
    "# Define the input layer\n",
    "inputs_1T = Input(shape=(360, num_features), batch_size=batch_size)\n",
    "inputs_5T = Input(shape=(360, num_features), batch_size=batch_size)\n",
    "inputs_15T = Input(shape=(360, num_features), batch_size=batch_size)\n",
    "inputs_1H = Input(shape=(360, num_features), batch_size=batch_size)\n",
    "inputs_4H = Input(shape=(360, num_features), batch_size=batch_size)\n",
    "inputs_1B = Input(shape=(360, num_features), batch_size=batch_size)\n",
    "\n",
    "main_inputs = [inputs_1T, inputs_5T, inputs_15T, inputs_1H, inputs_4H, inputs_1B]\n",
    "\n",
    "# Apply batch normalization to each input\n",
    "inputs_1T_norm = BatchNormalization()(inputs_1T)\n",
    "inputs_5T_norm = BatchNormalization()(inputs_5T)\n",
    "inputs_15T_norm = BatchNormalization()(inputs_15T)\n",
    "inputs_1H_norm = BatchNormalization()(inputs_1H)\n",
    "inputs_4H_norm = BatchNormalization()(inputs_4H)\n",
    "inputs_1B_norm = BatchNormalization()(inputs_1B)\n",
    "\n",
    "inputs = [inputs_1T_norm, inputs_5T_norm, inputs_15T_norm, inputs_1H_norm, inputs_4H_norm, inputs_1B_norm]\n",
    "\n",
    "# Define the Convolutional model\n",
    "timeframes = [1, 5, 15, 60, 240, 1440]  # in minutes\n",
    "conv_outputs = []\n",
    "for timeframe, inputs in zip(timeframes, inputs):\n",
    "    conv_layer = Conv1D(filters=128, kernel_size=5, strides=1, activation='sigmoid', kernel_initializer='glorot_uniform')(inputs)\n",
    "    conv_layer = BatchNormalization()(conv_layer)\n",
    "    conv_layer = Dropout(0.5)(conv_layer)\n",
    "\n",
    "    # Add LSTM layers\n",
    "    #x = LSTM(32, return_sequences=True)(conv_layer)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    \n",
    "    # Add transformer layers\n",
    "    x = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=4, kernel_initializer='glorot_uniform')(conv_layer, conv_layer, conv_layer)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    conv_outputs.append(x)\n",
    "    \n",
    "# Concatenate the output\n",
    "conv_output = Concatenate(axis=1)(conv_outputs)\n",
    "\n",
    "# Reshape the output to have a 3D shape\n",
    "#conv_output = Reshape((-1, len(timeframes) * 32))(conv_output)\n",
    "\n",
    "# Define the output layer\n",
    "#output1 = TimeDistributed(Dense(1), name='output1')(conv_output)\n",
    "#output2 = TimeDistributed(Dense(1), name='output2')(conv_output)\n",
    "output1 = GlobalAveragePooling1D()(conv_output)\n",
    "output2 = GlobalAveragePooling1D()(conv_output)\n",
    "\n",
    "output = Concatenate()([output1, output2])\n",
    "output = Dense(2)(output)\n",
    "\n",
    "# Define the model with two outputs\n",
    "model = Model(inputs=main_inputs, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'], run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62cdf476-8e13-4688-ae54-1965bf5048f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()\n",
    "#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8915ca-e6cb-4d76-b9d7-3cd875d63e2d",
   "metadata": {},
   "source": [
    "#### layer activations\n",
    "#### tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6f544f-2572-482a-95bf-5d89b049587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 20:45:40.065001: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 90s 1s/step - loss: 6989559.0000 - mae: 2438.2375 - val_loss: 6493355.5000 - val_mae: 2331.7947\n",
      "Epoch 2/30\n",
      "78/78 [==============================] - 94s 1s/step - loss: 5551296.5000 - mae: 2114.6008 - val_loss: 3864983.2500 - val_mae: 1674.6182\n",
      "Epoch 3/30\n",
      "78/78 [==============================] - 87s 1s/step - loss: 3221240.7500 - mae: 1480.6639 - val_loss: 2443137.0000 - val_mae: 1219.2311\n",
      "Epoch 4/30\n",
      "78/78 [==============================] - 82s 1s/step - loss: 1419072.0000 - mae: 822.1678 - val_loss: 1265580.3750 - val_mae: 798.4820\n",
      "Epoch 5/30\n",
      "78/78 [==============================] - 82s 1s/step - loss: 735150.8750 - mae: 525.3757 - val_loss: 479110.9375 - val_mae: 427.1653\n",
      "Epoch 6/30\n",
      "78/78 [==============================] - 85s 1s/step - loss: 366193.8125 - mae: 340.0566 - val_loss: 263885.5938 - val_mae: 309.4507\n",
      "Epoch 7/30\n",
      "78/78 [==============================] - 80s 1s/step - loss: 186480.6875 - mae: 245.8672 - val_loss: 170457.8906 - val_mae: 243.3963\n",
      "Epoch 8/30\n",
      "78/78 [==============================] - 83s 1s/step - loss: 107065.5234 - mae: 193.9855 - val_loss: 78786.5859 - val_mae: 174.0752\n",
      "Epoch 9/30\n",
      "78/78 [==============================] - 83s 1s/step - loss: 66426.6797 - mae: 159.5058 - val_loss: 41386.0820 - val_mae: 126.4630\n",
      "Epoch 10/30\n",
      "78/78 [==============================] - 83s 1s/step - loss: 39647.9062 - mae: 119.6714 - val_loss: 31125.6016 - val_mae: 110.8413\n",
      "Epoch 11/30\n",
      "78/78 [==============================] - 86s 1s/step - loss: 31123.4180 - mae: 114.1813 - val_loss: 26339.7949 - val_mae: 107.2729\n",
      "Epoch 12/30\n",
      "78/78 [==============================] - 81s 1s/step - loss: 26422.6836 - mae: 109.5810 - val_loss: 28932.7051 - val_mae: 121.7301\n",
      "Epoch 13/30\n",
      "78/78 [==============================] - 83s 1s/step - loss: 19449.9141 - mae: 92.6938 - val_loss: 19660.7461 - val_mae: 100.9976\n",
      "Epoch 14/30\n",
      "78/78 [==============================] - 80s 1s/step - loss: 16055.1846 - mae: 84.9605 - val_loss: 16142.6514 - val_mae: 86.7473\n",
      "Epoch 15/30\n",
      "78/78 [==============================] - 86s 1s/step - loss: 15299.1182 - mae: 83.4306 - val_loss: 13489.1426 - val_mae: 80.9699\n",
      "Epoch 16/30\n",
      "78/78 [==============================] - 81s 1s/step - loss: 12986.4365 - mae: 77.3521 - val_loss: 12673.5693 - val_mae: 84.2771\n",
      "Epoch 17/30\n",
      "78/78 [==============================] - 84s 1s/step - loss: 11599.9404 - mae: 72.4930 - val_loss: 19385.0801 - val_mae: 98.3029\n",
      "Epoch 18/30\n",
      "78/78 [==============================] - 80s 1s/step - loss: 9579.5967 - mae: 67.0451 - val_loss: 10116.8457 - val_mae: 73.5267\n",
      "Epoch 19/30\n",
      "78/78 [==============================] - 81s 1s/step - loss: 8499.5449 - mae: 64.4671 - val_loss: 7971.5034 - val_mae: 64.8094\n",
      "Epoch 20/30\n",
      "78/78 [==============================] - 86s 1s/step - loss: 6372.2822 - mae: 52.2752 - val_loss: 9951.6143 - val_mae: 73.3450\n",
      "Epoch 21/30\n",
      "78/78 [==============================] - 81s 1s/step - loss: 5674.7651 - mae: 49.0353 - val_loss: 9742.5986 - val_mae: 70.1736\n",
      "Epoch 22/30\n",
      "78/78 [==============================] - 93s 1s/step - loss: 8051.3740 - mae: 63.8291 - val_loss: 15828.2881 - val_mae: 101.4059\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 30\n",
    "lr = 1e-2\n",
    "lr_schedule = OneCycleScheduler(lr, train_steps_per_epoch)\n",
    "\n",
    "log_dir = \"logs/fit/\" + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Define the Early Stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor for early stopping\n",
    "    patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore the best model weights based on the monitored metric\n",
    ")\n",
    "\n",
    "# Define the ModelCheckpoint callback to save the best model during training\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model.h5',  # Filepath to save the best model\n",
    "    monitor='val_loss',  # Metric to monitor for saving the best model\n",
    "    save_best_only=True  # Save only the best model based on the monitored metric\n",
    ")\n",
    "\n",
    "history = model.fit(train_generator, steps_per_epoch=train_steps_per_epoch, epochs=epochs,\n",
    "          validation_data=val_generator, validation_steps=val_steps, callbacks=[lr_schedule, tensorboard_callback, early_stopping, checkpoint])\n",
    "\n",
    "# Plot the training and validation loss and MAE\n",
    "plt.semilogy(history.history['mae'], label='train_mae')\n",
    "plt.semilogy(history.history['val_mae'], label='val_mae')\n",
    "plt.ylim(ymin=1)  # Set the y-axis minimum value to 0\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64bc669d-9dc7-4fde-9ab5-96b7761eb9e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (417011211.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    18/18 [==============================] - 146s 8s/step - loss: 1.8419e-04 - mae: 0.0107 - val_loss: 8.4897e-05 - val_mae: 0.0076\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "Epoch 10/10\n",
    "18/18 [==============================] - 146s 8s/step - loss: 1.8419e-04 - mae: 0.0107 - val_loss: 8.4897e-05 - val_mae: 0.0076\n",
    "\n",
    "Epoch 3/10 - tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=4)(x, x, x)\n",
    "144/144 [==============================] - 24s 165ms/step - loss: 8.0584e-04 - mae: 0.0217 - val_loss: 3.0579e-05 - val_mae: 0.0041\n",
    "\n",
    "Epoch 18/10 - tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=4)(x, x, x) (No multiheadattention dropout)\n",
    "144/144 [==============================] - 33s 233ms/step - loss: 3.2141e-05 - mae: 0.0039 - val_loss: 1.5543e-05 - val_mae: 0.0025\n",
    "\n",
    "Epoch 5/30\n",
    "144/144 [==============================] - 33s 230ms/step - loss: 2.5526e-04 - mae: 0.0129 - val_loss: 2.5898e-05 - val_mae: 0.0039\n",
    "\n",
    "Epoch 11/30 - tf.keras.layers.MultiHeadAttention(num_heads=24, key_dim=64)(x, x, x) (No multiheadattention dropout)\n",
    "144/144 [==============================] - 79s 546ms/step - loss: 4.5514e-05 - mae: 0.0047 - val_loss: 1.5869e-05 - val_mae: 0.0026\n",
    "\n",
    "\n",
    "Epoch 21/30 - tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x, x) - batchnorm after inputs\n",
    "144/144 [==============================] - 38s 262ms/step - loss: 2.1482e-05 - mae: 0.0028 - val_loss: 1.6468e-05 - val_mae: 0.0024\n",
    "\n",
    "Epoch 18/30 - same as above but both LSTMs are 32\n",
    "140/140 [==============================] - 83s 593ms/step - loss: 2.0085e-05 - mae: 0.0028 - val_loss: 1.9584e-05 - val_mae: 0.0028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1eeb2-2889-4724-b407-90f73413ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82954b-8da6-4f80-845e-d46db43388d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = tf.keras.models.load_model('best_model.h5')\n",
    "test_steps = len(test_indices) // batch_size\n",
    "\n",
    "# Step 4: Calculate Importance\n",
    "def calculate_feature_importance(model, data_generator, metric, num_samples):\n",
    "    # Get the initial performance of the model\n",
    "    initial_predictions = model.predict(data_generator, steps=test_steps)\n",
    "    initial_score = metric(get_targets(data_generator), initial_predictions)\n",
    "\n",
    "    num_features = df_1B.shape[1]\n",
    "    importance_scores = np.zeros(num_features)\n",
    "\n",
    "    for feature_idx in range(num_features):\n",
    "        # Shuffle the values of the selected feature\n",
    "        shuffled_generator = data_generator.shuffle_feature(feature_idx)\n",
    "\n",
    "        # Generate num_samples samples with the shuffled feature\n",
    "        X_shuffled, y_shuffled = generate_samples(shuffled_generator, num_samples)\n",
    "\n",
    "        # Make predictions using the shuffled data\n",
    "        shuffled_predictions = model.predict(X_shuffled, steps=len(X_shuffled) // batch_size)\n",
    "\n",
    "        # Calculate the performance metric with the shuffled predictions\n",
    "        shuffled_score = metric(y_shuffled, shuffled_predictions)\n",
    "\n",
    "        # Calculate the feature importance as the drop in performance\n",
    "        importance_scores[feature_idx] = initial_score - shuffled_score\n",
    "\n",
    "    return importance_scores\n",
    "\n",
    "# Helper function to generate samples from the shuffled data generator\n",
    "def generate_samples(data_generator, num_samples):\n",
    "    X_samples = []\n",
    "    y_samples = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        X_sample, y_sample = next(data_generator)\n",
    "        X_samples.append(X_sample)\n",
    "        y_samples.append(y_sample)\n",
    "\n",
    "    X_samples = np.concatenate(X_samples)\n",
    "    y_samples = np.concatenate(y_samples)\n",
    "\n",
    "    return X_samples, y_samples\n",
    "\n",
    "def get_targets(data_generator):\n",
    "    targets = []\n",
    "    for i in range(len(test_indices)):\n",
    "        _ , y = next(data_generator)\n",
    "        targets.append(y)\n",
    "    return targets\n",
    "\n",
    "# Step 5: Repeat and Average\n",
    "def repeat_and_average(model, data_generator, metric, num_iterations, num_samples):\n",
    "    num_features = df_1B.shape[1]\n",
    "    importance_scores_sum = np.zeros(num_features)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        importance_scores = calculate_feature_importance(model, data_generator, metric, num_samples)\n",
    "        importance_scores_sum += importance_scores\n",
    "\n",
    "    # Calculate the average importance scores across iterations\n",
    "    importance_scores_avg = importance_scores_sum / num_iterations\n",
    "\n",
    "    return importance_scores_avg\n",
    "\n",
    "# Step 6: Rank the Features\n",
    "def rank_features(importance_scores):\n",
    "    feature_indices = np.argsort(importance_scores)[::-1]\n",
    "    ranked_features = [(index, importance_scores[index]) for index in feature_indices]\n",
    "\n",
    "    return ranked_features\n",
    "\n",
    "# Assuming you have your data generator ready (data_generator), you can use the following code to calculate feature importance:\n",
    "\n",
    "# Calculate feature importance\n",
    "num_iterations = 1  # Number of iterations for averaging importance scores\n",
    "num_samples = 1  # Number of samples to generate for each feature permutation\n",
    "importance_scores = repeat_and_average(model, test_generator, mean_squared_error, num_iterations, num_samples)\n",
    "\n",
    "# Rank the features\n",
    "ranked_features = rank_features(importance_scores)\n",
    "\n",
    "# Print the ranked features\n",
    "for feature_idx, importance_score in ranked_features:\n",
    "    print(f\"Feature #{feature_idx + 1}: Importance Score = {importance_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ee1d783-e177-4937-9a67-ef5825dbd7f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute '__copy__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__copy__\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute '__copy__'"
     ]
    }
   ],
   "source": [
    "test_generator.__copy__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ead58-3e78-4192-abc4-5f6bc2b08ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = data_generator(df_1T, window_size, batch_size, test_indices)\n",
    "test_steps = len(test_indices) // batch_size\n",
    "# Generate predictions for the test set\n",
    "predictions = model.predict(test_generator, steps=test_steps)\n",
    "test_generator = data_generator(df_1T, window_size, batch_size, test_indices)\n",
    "model.evaluate(test_generator, steps=test_steps)\n",
    "\n",
    "#dates = df.iloc[test_indices].index.strftime(\"%Y-%m-%d\")[:576]\n",
    "\n",
    "# Extract the predicted high and low values\n",
    "predicted_high = predictions[:, 0]\n",
    "predicted_low = predictions[:, 1]\n",
    "\n",
    "# Reset the test data generator to get actual values\n",
    "test_generator = data_generator(df_1T, window_size, batch_size, test_indices, test=True)\n",
    "\n",
    "# Get the actual high and low values\n",
    "actual_high = []\n",
    "actual_low = []\n",
    "dates = df_1T.iloc[test_indices].index\n",
    "x_test = []\n",
    "for i in range(test_steps):\n",
    "    x, batch_targets = next(test_generator)\n",
    "    x_test.append(x)\n",
    "    actual_high.extend(batch_targets[:, 0])\n",
    "    actual_low.extend(batch_targets[:, 1])\n",
    "    \n",
    "    \n",
    "# actual_high = np.exp(actual_high) * open_prices\n",
    "# actual_low = np.exp(actual_low) * open_prices\n",
    "# predicted_high = np.exp(predicted_high) * open_prices\n",
    "# predicted_low = np.exp(predicted_low) * open_prices\n",
    "\n",
    "# Print the individual predictions with actual values\n",
    "# for i in range(len(predicted_high)):\n",
    "#     print(f\"Sample {i+1} - {dates[i]} - Predicted/Actual High: {predicted_high[i]:.2f}, {actual_high[i]:.2f} Predicted/Actual Low: {predicted_low[i]:.2f}, {actual_low[i]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5a2511e-6486-4c19-bb04-6bdbc738f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_es = pd.read_csv('es-1m_bk.csv', sep=';', names=['date', 'time', 'open', 'high', 'low', 'close', 'vol'])\n",
    "df_es['datetime'] = pd.to_datetime(df_es.date + ' ' + df_es.time, format='%d/%m/%Y %H:%M')\n",
    "df_es = df_es.infer_objects()\n",
    "df_es.set_index(df_es.datetime, inplace=True)\n",
    "df_es.index = df_es.index.tz_localize('America/Chicago').tz_convert('America/New_York')\n",
    "df_es.drop(columns=['date', 'time', 'datetime', 'vol'], inplace=True)\n",
    "df_es = df_es[~df_es.index.duplicated(keep='last')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c1202dd-f3bd-4bdd-b07c-e63a8ac73f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-11-05 1690.3889 1698.2906 1680.7097 1689.3636\n",
      "2022-06-29 4021.2473 3920.2227 3984.2542 3880.663\n",
      "2016-02-04 1891.1135 1919.5045 1884.5809 1891.5372\n",
      "2015-11-10 1953.7242 2064.5012 1946.3475 2054.0542\n",
      "2016-11-08 2082.936 2161.5842 2075.5056 2142.1243\n",
      "2022-03-25 4352.5317 4633.8257 4315.212 4587.1196\n",
      "2017-04-18 2354.1582 2378.6548 2344.175 2365.719\n",
      "2023-01-19 3952.389 3965.971 3914.7996 3933.709\n",
      "2016-05-31 2086.5173 2109.3171 2078.8132 2100.0327\n",
      "2022-11-01 4004.5593 3988.8186 3966.6843 3915.888\n",
      "2017-08-02 2552.015 2514.399 2540.082 2501.7039\n",
      "2011-12-23 1171.9885 1177.3469 1161.9968 1171.0221\n",
      "2017-12-21 2746.6707 2735.159 2730.4626 2726.0208\n",
      "2015-01-29 1962.9363 1967.0001 1954.5142 1948.0796\n",
      "2023-03-22 4061.578 4047.5 4023.3 4029.0\n",
      "2014-10-08 1850.9159 1894.3276 1842.7634 1878.8988\n",
      "2018-10-23 2833.7427 2751.866 2811.6272 2720.2905\n",
      "2018-09-04 2927.3855 2936.2598 2905.149 2921.3306\n",
      "2019-11-27 3164.414 3162.0676 3139.9993 3156.292\n",
      "2011-09-28 1112.1592 1099.8135 1101.4188 1084.4249\n",
      "2022-09-06 4103.128 4026.9058 4065.887 3967.9492\n",
      "2021-04-19 4212.3096 4225.804 4174.442 4200.2256\n",
      "2012-05-07 1259.5107 1285.8259 1249.1069 1279.2379\n",
      "2011-10-05 1133.0508 1052.9484 1122.6697 1034.9951\n",
      "2016-12-20 2156.664 2299.893 2148.5247 2293.5593\n",
      "2016-12-16 2147.1638 2293.0525 2139.3176 2282.1587\n",
      "2017-10-04 2496.0374 2578.4365 2485.361 2570.306\n",
      "2013-08-22 1536.4485 1590.3195 1526.1292 1579.7445\n",
      "2013-10-18 1685.5552 1675.6113 1675.7708 1669.8209\n",
      "2017-12-11 2631.6807 2701.398 2617.57 2695.3057\n",
      "2019-07-11 2919.306 3022.211 2896.6306 3011.4038\n",
      "2019-11-26 3143.9888 3153.0273 3119.8542 3143.736\n",
      "2012-12-05 1263.5172 1341.7208 1253.0667 1327.465\n",
      "2015-11-04 1895.9995 2097.8318 1888.5537 2082.9075\n",
      "2020-02-14 3274.116 3392.8914 3248.155 3382.3538\n",
      "2017-11-21 2598.1414 2641.4492 2584.877 2631.54\n",
      "2019-05-09 2854.3003 2878.3953 2833.6062 2855.491\n",
      "2022-09-29 3894.8342 3758.3376 3857.3538 3690.9976\n",
      "2013-02-14 1328.6112 1449.5326 1317.8511 1441.8972\n",
      "2020-04-21 2759.0007 2781.0347 2736.0386 2721.459\n",
      "2016-07-07 2012.7437 2118.5964 2005.0839 2105.7456\n",
      "2019-04-30 2888.2874 2961.7065 2867.446 2945.8499\n",
      "2014-10-13 1818.4491 1867.1434 1810.4702 1845.5919\n",
      "2023-01-16 3922.7183 4053.4316 3885.4766 4043.3496\n",
      "2013-10-14 1455.3003 1636.7671 1445.2792 1626.6338\n",
      "2020-08-27 3387.4238 3516.2554 3361.0906 3491.6294\n",
      "2022-06-20 3931.8152 3802.82 3895.068 3774.4905\n",
      "2018-04-20 2687.156 2731.1868 2667.6858 2704.3286\n",
      "2012-05-18 1235.1687 1233.3575 1224.8994 1219.9464\n",
      "2017-09-14 2337.1785 2537.021 2327.7317 2529.6526\n",
      "2016-12-09 2077.3591 2279.1184 2069.2495 2272.5315\n",
      "2019-11-25 3084.3362 3144.9917 3061.2583 3133.1892\n",
      "2019-01-07 2632.8489 2584.9956 2612.485 2546.9106\n",
      "2021-02-24 3799.8567 3945.725 3764.6963 3896.2078\n",
      "2018-05-11 2745.2942 2769.4473 2725.5066 2757.0317\n",
      "2013-11-06 1596.3601 1708.1826 1586.8877 1698.5319\n",
      "2020-08-17 3264.7942 3400.161 3238.8254 3391.366\n",
      "2021-06-15 4304.493 4312.687 4267.378 4292.384\n",
      "2023-05-01 4121.3086 4201.5 4083.4932 4184.0\n",
      "2018-12-20 2565.407 2534.2996 2544.7468 2485.1167\n",
      "2013-04-08 1414.7532 1483.2086 1404.0654 1477.7019\n",
      "2018-07-09 2864.172 2817.838 2843.772 2806.4512\n",
      "2018-02-05 2687.346 2805.4736 2671.2942 2776.2817\n",
      "2011-12-05 1107.6725 1180.7198 1097.4465 1174.1914\n",
      "2017-04-27 2385.0037 2422.2812 2374.8704 2413.4038\n",
      "2019-01-30 2713.1702 2686.388 2691.549 2669.9937\n",
      "2016-09-05 2067.3518 2196.2043 2060.8098 2192.1729\n",
      "2020-11-16 3466.7722 3653.5283 3436.2744 3626.5566\n",
      "2023-02-09 3969.693 4200.8794 3931.6792 4163.8286\n",
      "2022-04-11 4274.604 4552.6646 4237.0034 4517.444\n",
      "2013-12-03 1690.4487 1735.6873 1680.63 1726.519\n",
      "2018-05-01 2688.1257 2678.4836 2667.245 2662.014\n",
      "2021-02-11 3696.9263 3961.894 3662.4944 3945.9778\n",
      "2012-02-01 1219.3308 1240.5947 1209.1702 1233.8015\n",
      "2015-10-26 1907.9941 2056.7903 1901.6017 2049.0796\n",
      "2013-09-23 1612.3965 1642.7988 1601.8359 1630.4941\n",
      "2019-11-14 2996.884 3109.333 2973.3752 3095.5215\n",
      "2022-03-24 4333.8213 4592.4795 4296.5747 4549.092\n",
      "2018-11-07 2811.3904 2827.142 2789.4412 2803.1445\n",
      "2012-02-16 1255.0498 1267.2994 1245.0239 1254.1813\n",
      "2020-05-27 3016.3235 3022.842 2991.7654 2969.274\n",
      "2012-07-02 1305.191 1287.5153 1295.0342 1276.6362\n",
      "2014-05-06 1835.0648 1823.0409 1825.4241 1816.2367\n",
      "2012-11-30 1341.9829 1346.2352 1331.7219 1340.0576\n",
      "2011-11-01 1116.4501 1145.979 1106.1925 1128.0258\n",
      "2012-02-23 1294.1194 1274.7954 1284.1887 1265.1912\n",
      "2014-08-11 1918.3552 1893.8137 1910.4989 1885.0325\n",
      "2020-04-09 2565.2356 2807.0676 2542.5369 2769.0193\n",
      "2016-12-12 2132.2256 2289.2524 2124.9114 2279.8784\n",
      "2018-02-02 2723.6477 2850.9114 2706.7727 2829.5886\n",
      "2017-12-08 2622.3538 2692.5134 2608.6643 2687.1826\n",
      "2023-01-30 3992.0215 4111.4023 3954.1624 4078.6362\n",
      "2017-12-27 2719.5984 2732.3667 2703.5525 2727.2898\n",
      "2023-02-17 3971.0273 4121.2324 3932.9392 4089.7263\n",
      "2015-11-03 1929.5525 2092.3596 1922.6719 2079.9229\n",
      "2017-10-23 2494.087 2618.8357 2483.8967 2612.2295\n",
      "2020-10-21 3379.4368 3485.9006 3350.5784 3452.8792\n",
      "2018-03-12 2730.6838 2840.1404 2711.0044 2821.3904\n",
      "2014-09-05 1910.2305 1949.428 1901.8977 1939.915\n",
      "2017-07-13 2507.4438 2484.1846 2496.5122 2477.0752\n",
      "2020-03-03 2811.2212 3148.266 2784.0913 3034.3584\n",
      "2013-11-07 1659.0 1709.6302 1649.5056 1691.0526\n",
      "2011-05-16 1167.6777 1240.0015 1157.4474 1227.7517\n",
      "2019-10-02 2887.0686 2936.8154 2862.1836 2895.381\n",
      "2014-01-29 1809.5796 1724.8617 1799.6567 1712.7557\n",
      "2017-09-18 2459.7605 2546.9302 2449.8105 2540.324\n",
      "2016-05-18 2007.1611 2059.884 2000.7201 2043.8245\n",
      "2023-02-23 4076.0564 4067.2942 4038.1194 4008.063\n",
      "2021-01-12 3744.2595 3843.406 3709.5134 3821.4265\n",
      "2017-04-06 2369.1975 2392.098 2359.3958 2379.162\n",
      "2018-07-23 2855.4893 2839.599 2832.2202 2829.9836\n",
      "2015-10-09 1864.644 2002.0687 1858.7456 1992.368\n",
      "2022-12-15 4061.3638 4017.8928 4023.5845 3949.0837\n",
      "2017-09-11 2479.437 2524.0627 2469.4648 2514.4075\n",
      "2019-02-15 2643.943 2797.6165 2621.4873 2787.5278\n",
      "2013-12-09 1711.4446 1748.2333 1701.5758 1743.1666\n",
      "2014-01-08 1727.3821 1776.9175 1717.451 1767.959\n",
      "2016-03-31 1972.0693 2067.161 1965.5728 2061.8914\n",
      "2012-08-03 1279.3129 1314.713 1269.4612 1304.3069\n",
      "2013-11-18 1651.4728 1736.8937 1641.7117 1729.8969\n",
      "2016-12-05 2094.0588 2232.8538 2087.2766 2222.7446\n",
      "2017-02-08 2330.7239 2321.1743 2321.138 2311.5469\n",
      "2014-02-13 1769.1781 1764.3273 1759.4937 1746.1682\n",
      "2016-05-05 2038.6707 2062.6443 2031.9901 2053.109\n",
      "2018-03-23 2655.59 2694.7002 2636.452 2661.254\n",
      "2015-02-20 1961.2759 2057.6714 1953.3257 2046.6139\n",
      "2021-09-30 4407.1924 4449.307 4370.0938 4394.88\n",
      "2021-10-07 4466.285 4498.139 4429.1465 4462.787\n",
      "2016-06-08 1960.6321 2126.6313 1951.6211 2119.1035\n",
      "2016-10-18 1971.5372 2162.5952 1962.7485 2152.2334\n",
      "2016-10-28 1976.2948 2158.0461 1967.7339 2144.9043\n",
      "2020-01-21 2977.946 3340.7046 2957.093 3327.909\n",
      "2017-03-03 2137.7449 2413.8997 2127.936 2406.046\n",
      "2019-04-12 2647.2568 2934.5234 2632.2014 2922.1904\n",
      "2017-04-20 2207.5745 2383.2205 2197.6877 2371.299\n",
      "2017-10-25 2156.1501 2606.8938 2147.5608 2587.8376\n",
      "2016-08-15 1953.9501 2208.047 1945.3901 2201.7478\n",
      "2023-01-24 3510.7942 4070.0667 3475.7034 4038.0566\n",
      "2023-02-22 3570.6824 4051.415 3534.6013 4022.6816\n",
      "2017-09-27 2067.3645 2545.9138 2059.09 2533.7178\n",
      "2015-06-12 1883.5844 2073.2876 1875.9297 2061.4062\n",
      "2014-06-16 1736.5708 1886.496 1728.2217 1876.0073\n",
      "2021-10-26 4465.162 4669.56 4428.1606 4652.0107\n",
      "2017-07-25 2122.8904 2517.192 2114.2156 2511.0981\n",
      "2021-06-23 4206.604 4310.6567 4169.156 4298.4746\n",
      "2021-12-15 4414.352 4719.077 4377.179 4688.768\n",
      "2021-07-22 4229.621 4424.6094 4192.4897 4409.8896\n",
      "2018-06-05 2634.5237 2789.7178 2617.7185 2776.0352\n",
      "2018-01-18 2473.8655 2849.6423 2462.5496 2836.6963\n",
      "2015-12-15 1901.5565 2039.3644 1893.8623 2027.1287\n",
      "2021-05-10 3876.0498 4286.0786 3838.8882 4269.111\n",
      "2015-06-24 1890.452 2096.5552 1882.6847 2087.8916\n",
      "2013-06-19 1378.02 1583.1093 1367.7542 1577.3412\n",
      "2017-11-15 2317.1257 2613.5 2307.7458 2597.2388\n",
      "2022-03-21 4292.1543 4566.447 4254.2676 4528.1636\n",
      "2022-12-01 3584.5208 4177.625 3549.5935 4121.2114\n",
      "2021-03-12 3463.2588 3974.829 3430.8757 3955.3284\n",
      "2018-03-08 2567.7417 2779.3276 2553.9094 2766.8894\n",
      "2018-10-29 2610.9763 2735.194 2596.4192 2705.387\n",
      "2011-09-23 1047.6396 1057.8447 1036.9956 1038.7257\n",
      "2015-04-30 2034.3845 2069.3545 2027.0288 2055.0525\n",
      "2013-07-22 1525.4811 1627.5719 1514.9658 1620.3618\n",
      "2022-03-11 4132.1504 4370.691 4093.9504 4319.391\n",
      "2014-09-18 1875.3796 1963.3904 1866.6052 1956.5332\n",
      "2017-02-15 2257.8806 2374.3772 2248.7974 2363.23\n",
      "2022-10-07 3321.406 3778.6667 3287.2847 3731.1475\n",
      "2018-08-30 2912.8372 2948.4058 2893.0623 2939.2964\n",
      "2015-05-26 1960.9989 2090.3142 1953.9362 2071.5737\n",
      "2012-02-29 1321.236 1290.4902 1311.2822 1277.138\n",
      "2012-11-20 1332.611 1318.4362 1322.3347 1309.8826\n",
      "2022-05-25 3733.669 4060.5952 3696.3264 4003.4253\n",
      "2014-09-29 1881.7025 1932.5326 1872.9612 1915.6343\n",
      "2012-11-27 1339.3247 1336.9689 1329.1957 1328.4153\n",
      "2022-04-14 4095.7517 4548.8364 4057.6726 4505.1934\n",
      "2022-11-23 3368.2312 4104.44 3336.2488 4070.8972\n",
      "2015-05-12 1954.8795 2068.3682 1948.0549 2051.8467\n",
      "2021-10-21 4335.897 4612.0806 4299.2744 4596.0576\n",
      "2017-02-16 2266.3096 2381.2178 2257.2944 2368.0437\n",
      "2021-09-09 4281.542 4598.203 4244.221 4576.377\n",
      "2015-02-04 1971.8215 2011.2301 1963.5175 1998.4525\n",
      "2017-06-16 2312.4116 2470.4736 2303.5522 2458.0325\n",
      "2022-02-16 4315.0923 4535.6934 4277.102 4509.2046\n",
      "2018-09-19 2885.0208 2949.1497 2866.7698 2940.3086\n",
      "2021-12-17 4292.627 4744.5474 4255.7603 4676.2876\n",
      "2019-09-12 2942.604 3036.5366 2922.7659 3016.933\n",
      "2014-06-27 1862.4578 1902.5948 1854.0159 1897.4725\n",
      "2018-02-28 2703.4785 2804.4583 2686.8535 2779.5815\n",
      "2020-11-20 3444.2283 3607.1472 3416.4248 3591.5188\n",
      "2013-04-26 1483.4128 1514.5726 1473.1318 1506.6718\n",
      "2014-05-23 1827.064 1843.6965 1817.9194 1837.8644\n",
      "2018-08-06 2851.8884 2882.1096 2832.4392 2869.9639\n",
      "2012-02-21 1276.6243 1279.012 1266.6697 1273.1556\n",
      "2019-11-01 2929.1907 3075.6833 2904.2195 3063.8809\n",
      "2019-06-24 2779.5208 2975.7153 2755.7874 2968.4268\n",
      "2020-03-18 2402.3093 2445.1077 2378.3252 2346.232\n",
      "2018-05-03 2655.8472 2659.987 2634.4932 2626.2874\n",
      "2012-10-31 1338.505 1344.3345 1328.2062 1332.9297\n",
      "2020-06-09 3036.982 3218.841 3011.3801 3194.8105\n",
      "2012-12-27 1329.4751 1353.8514 1318.7418 1337.8649\n",
      "2017-12-13 2714.2908 2716.6284 2699.0017 2711.044\n",
      "2012-11-19 1323.5596 1311.3082 1313.1793 1303.4674\n",
      "2021-08-12 4373.033 4512.1675 4335.6616 4497.4478\n",
      "2017-04-10 2386.5234 2397.678 2376.4075 2384.9958\n",
      "2015-11-16 1929.2505 2019.9777 1922.4113 2004.3073\n",
      "2017-03-02 2375.384 2425.0469 2364.8289 2414.4062\n",
      "2021-10-28 4489.3096 4664.2188 4452.22 4638.0225\n",
      "2019-03-28 2689.0383 2843.4094 2666.559 2822.5188\n",
      "2015-11-02 1897.1936 2077.684 1890.5062 2064.75\n",
      "2016-09-01 2067.5286 2189.4011 2061.145 2171.763\n",
      "2021-05-04 4136.005 4224.538 4098.05 4174.14\n",
      "2011-08-11 1080.1315 1071.2927 1069.7036 1042.2887\n",
      "2019-05-17 2637.953 2907.0886 2614.971 2871.851\n",
      "2015-12-10 2003.9 2052.0645 1997.1824 2033.4093\n",
      "2013-04-12 1461.3422 1519.1217 1450.6129 1508.3478\n",
      "2012-11-01 1332.4939 1353.3632 1322.1877 1339.1073\n",
      "2020-01-17 3235.0505 3338.4465 3205.759 3330.9197\n",
      "2014-05-01 1848.4417 1830.0881 1838.7074 1819.1528\n",
      "2020-02-27 2940.3313 3080.5237 2912.717 3016.7957\n",
      "2014-06-10 1878.4769 1894.4851 1869.5837 1888.6528\n",
      "2015-07-17 1947.5867 2097.05 1940.4895 2091.852\n",
      "2014-10-09 1853.3912 1920.7772 1845.2158 1893.593\n",
      "2012-08-31 1310.2552 1335.5251 1300.1287 1321.5715\n",
      "2022-02-08 4435.3164 4591.982 4398.1978 4540.0234\n",
      "2020-06-18 2910.6838 3125.002 2885.1335 3096.6064\n",
      "2015-04-24 2035.8082 2084.1497 2028.4453 2077.245\n",
      "2016-04-26 2014.4227 2099.029 2007.0972 2087.2354\n",
      "2013-02-19 1387.3802 1456.4521 1376.9751 1450.2484\n",
      "2014-07-14 1856.0767 1926.0115 1847.6709 1921.8647\n",
      "2020-05-13 2883.661 2872.401 2860.1035 2812.0742\n",
      "2015-03-25 2025.1599 2060.7239 2017.158 2038.2845\n",
      "2019-01-23 2712.9897 2677.3079 2692.6462 2639.223\n",
      "2012-02-22 1241.9429 1275.0297 1231.9055 1268.2365\n",
      "2011-12-27 1145.6526 1184.3744 1135.7532 1177.1127\n",
      "2017-06-29 2364.4956 2478.5986 2355.156 2451.9387\n",
      "2022-02-22 4314.4424 4440.6904 4276.695 4388.477\n",
      "2020-11-13 3456.2075 3595.0479 3426.968 3576.8987\n",
      "2014-02-04 1766.1133 1696.7758 1756.365 1683.9435\n",
      "2017-03-09 2261.22 2400.9788 2252.0007 2393.125\n",
      "2015-09-30 1830.357 1897.3512 1824.3737 1884.9144\n",
      "2021-07-06 4298.4644 4411.666 4260.8955 4375.374\n",
      "2015-05-01 2037.5116 2067.135 2030.144 2058.7512\n",
      "2017-07-07 2410.1833 2460.3176 2400.405 2449.6538\n",
      "2018-12-14 2649.3123 2663.1836 2627.9617 2632.665\n",
      "2020-02-12 3249.3333 3389.128 3223.5774 3380.5974\n",
      "2011-06-30 1170.8208 1221.6495 1161.058 1211.6721\n",
      "2014-05-19 1829.5026 1829.1161 1820.2273 1816.9657\n",
      "2021-04-16 4075.8628 4232.8955 4037.887 4215.9272\n",
      "2017-02-07 2358.3103 2325.7344 2348.035 2319.401\n",
      "2023-01-09 3651.085 4005.7944 3615.28 3960.678\n",
      "2016-10-04 2019.8817 2181.8025 2012.1304 2168.408\n",
      "2019-09-03 2893.1287 2929.9739 2871.3953 2906.3489\n",
      "2016-08-26 2007.5437 2204.0156 2000.5054 2185.1174\n",
      "2022-10-17 3672.7927 3763.42 3636.0505 3721.7454\n",
      "2021-07-05 4291.5967 4411.92 4254.113 4407.3516\n",
      "2022-04-29 4215.4194 4354.612 4177.957 4290.551\n",
      "2019-04-17 2778.1394 2943.3328 2757.6096 2921.9387\n",
      "2012-04-19 1258.6385 1304.8839 1248.469 1293.5902\n",
      "2019-05-16 2793.9758 2911.8708 2774.3948 2878.647\n",
      "2020-07-16 3153.243 3223.2551 3130.2346 3206.6702\n",
      "2013-09-11 1551.846 1620.602 1541.4631 1612.9113\n",
      "2020-10-26 3370.8694 3458.4248 3345.124 3407.5063\n",
      "2016-05-26 2061.0955 2099.531 2052.9702 2092.003\n",
      "2019-12-17 3057.8748 3215.005 3036.5085 3207.478\n",
      "2022-07-28 3958.818 4142.777 3921.6626 4077.9502\n",
      "2019-05-27 2803.3245 2849.9536 2783.5627 2846.6816\n",
      "2013-11-14 1643.6707 1726.0365 1634.1244 1715.1794\n",
      "2017-04-19 2378.4421 2383.4739 2367.637 2376.8794\n",
      "2022-07-15 3827.7751 3944.2136 3790.9539 3899.5496\n",
      "2016-04-14 2016.7092 2087.2354 2009.6785 2079.4565\n",
      "2018-09-25 2899.5586 2960.7695 2880.4568 2950.6653\n",
      "2013-07-19 1502.3005 1619.6407 1491.9045 1613.6323\n",
      "2023-03-24 3792.0906 3977.75 3754.4841 3937.5\n",
      "2021-12-22 4474.1797 4764.414 4437.2695 4722.643\n",
      "2023-01-11 3608.034 3999.2412 3573.2588 3981.0938\n",
      "2012-02-06 1200.4065 1254.6498 1190.0105 1249.7306\n",
      "2018-05-17 2753.6099 2768.9404 2734.1506 2750.1904\n",
      "2015-02-25 1958.9255 2078.312 1950.5635 2072.6604\n",
      "2022-01-11 4499.897 4775.8755 4462.855 4714.238\n",
      "2019-04-26 2819.4148 2954.6592 2799.5383 2939.054\n",
      "2021-04-20 3850.7126 4205.0376 3814.0422 4164.0103\n",
      "2015-07-27 1922.6732 2049.0298 1915.1692 2036.1584\n",
      "2013-03-18 1452.7766 1483.9269 1442.3615 1472.6741\n",
      "2022-08-19 3971.7288 4350.7837 3934.146 4313.7764\n",
      "2014-04-08 1810.8468 1795.0951 1801.1226 1779.5426\n",
      "2020-04-20 2736.387 2864.3906 2714.556 2822.5874\n",
      "2011-06-13 1163.8086 1180.1157 1154.169 1174.0829\n",
      "2013-02-25 1412.1377 1455.0205 1401.6039 1437.3636\n",
      "2015-12-16 1981.5236 2049.852 1974.4869 2035.1193\n"
     ]
    }
   ],
   "source": [
    "for d,ph,h,pl,l in zip(test_indices, predictions[:, 0], actual_high, predictions[:, 1], actual_low):\n",
    "    print(df_1T.iloc[d].name.date(), ph, h, pl, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb56c2-0a0b-4b1a-8395-35541b0f1ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
